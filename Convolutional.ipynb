{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import mnist\n",
    "from keras.datasets import fashion_mnist\n",
    "from pandas import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_images = mnist.train_images()\n",
    "train_labels = mnist.train_labels()\n",
    "test_images = mnist.test_images()\n",
    "test_labels = mnist.test_labels()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "B1 = 0.9\n",
    "B2 = 0.999\n",
    "EPS = 1e-8\n",
    "\n",
    "class Adam:\n",
    "    def __init__(self, nu = 1e-3):\n",
    "        self.nu = nu\n",
    "        self.m, self.v, self.t = 0, 0, 0\n",
    "    \n",
    "    def initialState(self, theta0):\n",
    "        self.theta = theta0\n",
    "        \n",
    "    def optimize(self, g):\n",
    "        self.m = B1 * self.m + (1 - B1) * g\n",
    "        self.v = B2 * self.v + (1 - B2) * (g ** 2)\n",
    "        self.t += 1\n",
    "        mMean = self.m / (1 - B1 ** self.t)\n",
    "        vMean = self.v / (1 - B2 ** self.t)\n",
    "        self.theta = self.theta - self.nu * (mMean / (np.sqrt(vMean) - EPS))\n",
    "        return self.theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdaGrad:\n",
    "    def __init__(self, nu = 1e-3):\n",
    "        self.G_sum = 0\n",
    "    \n",
    "    def initialState(self, theta0):\n",
    "        self.eps = np.full(theta0.shape, EPS)\n",
    "        \n",
    "    def optimize(self, g):\n",
    "        self.G_sum += g * g\n",
    "\n",
    "        self.g -=  / np.sqrt(self.G_sum + EPS)\n",
    "        return self.g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Conv3x3:\n",
    "    def __init__(self, filterCount, optimizer):\n",
    "        self.filterCount = filterCount\n",
    "        self.filters = np.random.randn(filterCount, 3, 3) / 9.0\n",
    "        self.optimizer = optimizer\n",
    "        optimizer.initialState(self.filters)\n",
    "    \n",
    "    def applyTo(self, image):\n",
    "        h, w = image.shape\n",
    "        output = np.zeros((h - 2, w - 2, self.filterCount))\n",
    "\n",
    "        for i in range(h - 2):\n",
    "            for j in range(w - 2):\n",
    "                block = image[i : (i + 3), j : (j + 3)]\n",
    "                output[i, j] = np.sum(block * self.filters, axis=(1, 2))\n",
    "\n",
    "        self.lastImage = image\n",
    "        return output\n",
    "\n",
    "    def optimize(self, dL_dout):\n",
    "        h, w = self.lastImage.shape\n",
    "        \n",
    "        dL_dfilters = np.zeros(self.filters.shape)\n",
    "        for i in range(h - 2):\n",
    "            for j in range(w - 2):\n",
    "                block = self.lastImage[i : (i + 3), j : (j + 3)]\n",
    "                for f in range(self.filterCount):\n",
    "                    dL_dfilters[f] += dL_dout[i, j, f] * block\n",
    "\n",
    "        self.filters = self.optimizer.optimize(dL_dfilters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MaxPool2:\n",
    "\n",
    "    def applyTo(self, image):\n",
    "        h, w, f = image.shape\n",
    "        output = np.zeros((h // 2, w // 2, f))\n",
    "\n",
    "        for i in range(h // 2):\n",
    "            for j in range(w // 2):\n",
    "                block = image[(2 * i) : (2 * i + 2), (2 * j) : (2 * j + 2)]\n",
    "                output[i, j] = np.amax(block, axis=(0, 1))\n",
    "        \n",
    "        self.lastImage = image\n",
    "        return output\n",
    "\n",
    "    \n",
    "    def optimize(self, dL_dout):\n",
    "        h, w, f = self.lastImage.shape\n",
    "        dL_din = np.zeros((h, w, f))\n",
    "\n",
    "        for i in range(h // 2):\n",
    "            for j in range(w // 2):\n",
    "                block = self.lastImage[(2 * i) : (2 * i + 2), (2 * j) : (2 * j + 2)]\n",
    "               \n",
    "                h, w, f = block.shape\n",
    "                mx = np.amax(block, axis=(0, 1))\n",
    "\n",
    "                for i2 in range(h):\n",
    "                    for j2 in range(w):\n",
    "                        for f2 in range(f):\n",
    "                            if block[i2, j2, f2] == mx[f2]:\n",
    "                                dL_din[2 * i + i2, 2 * j + j2, f2] = dL_dout[i, j, f2]\n",
    "        return dL_din"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Softmax:\n",
    "    def __init__(self, length, nodes, initOptimizer):\n",
    "        self.weights = np.random.randn(length, nodes) / length\n",
    "        self.biases = np.zeros(nodes)\n",
    "\n",
    "        self.weightsOptimizer = initOptimizer()\n",
    "        self.weightsOptimizer.initialState(self.weights)\n",
    "\n",
    "        self.biasesOptimizer = initOptimizer()\n",
    "        self.biasesOptimizer.initialState(self.biases)\n",
    "        \n",
    "    def applyTo(self, image):\n",
    "        self.lastImageShape = image.shape\n",
    "        image = image.flatten()\n",
    "        length, nodes = self.weights.shape\n",
    "        totals = np.dot(image, self.weights) + self.biases\n",
    "        exp = np.exp(totals)\n",
    "        \n",
    "        self.lastImage = image\n",
    "        self.lastTotals = totals\n",
    "\n",
    "        return exp / np.sum(exp, axis=0)\n",
    "\n",
    "    def optimize(self, dL_dout):\n",
    "        for i, grad in enumerate(dL_dout):\n",
    "            if grad == 0:\n",
    "                continue\n",
    "\n",
    "            totalExp = np.exp(self.lastTotals)\n",
    "            s = np.sum(totalExp)\n",
    "\n",
    "            dout_dtotals = -totalExp[i] * totalExp / (s ** 2)\n",
    "            dout_dtotals[i] = totalExp[i] * (s - totalExp[i]) / (s ** 2)\n",
    "\n",
    "            dtotals_dweights = self.lastImage\n",
    "            dtotals_dbiases = 1\n",
    "            dtotals_dimages = self.weights\n",
    "\n",
    "            dL_dtotals = grad * dout_dtotals\n",
    "\n",
    "            dL_dweights = np.matmul(dtotals_dweights[np.newaxis].T, dL_dtotals[np.newaxis])\n",
    "            dL_dbiases = dL_dtotals * dtotals_dbiases\n",
    "            dL_dimages = np.matmul(dtotals_dimages, dL_dtotals)\n",
    "\n",
    "                        \n",
    "            self.weights = self.weightsOptimizer.optimize(dL_dweights)\n",
    "            self.biases = self.biasesOptimizer.optimize(dL_dbiases)\n",
    "\n",
    "            return dL_dimages.reshape(self.lastImageShape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "H, W = train_images[0].shape\n",
    "D = 4\n",
    "LABELS = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model:\n",
    "    def __init__(self, *layers):\n",
    "        self.layers = layers\n",
    "        \n",
    "        \n",
    "    def _processImage(self, image):\n",
    "        image = (image / 255) - 0.5\n",
    "        for layer in self.layers:\n",
    "            image = layer.applyTo(image)\n",
    "        return image\n",
    "        \n",
    "    def _optimize(self, output, c):\n",
    "        grad = np.zeros(10)\n",
    "        grad[c] = -1 / output[c]\n",
    "\n",
    "        for layer in reversed(self.layers):\n",
    "            grad = layer.optimize(grad)\n",
    "        \n",
    "    def fit(self, images, classes):\n",
    "        totalLoss, totalCorrect = 0, 0\n",
    "        \n",
    "        for i, (image, c) in enumerate(zip(images, classes)):\n",
    "            if i % 100 == 0:\n",
    "                print('Step: {0}, Loss: {1}, Accuracy: {2}%'.format(i, totalLoss / 100, totalCorrect))\n",
    "                totalLoss, totalCorrect = 0, 0\n",
    "                \n",
    "            output = self._processImage(image)\n",
    "            self._optimize(output, c)\n",
    "            \n",
    "            totalLoss -= np.log(output[c])\n",
    "            totalCorrect += (np.argmax(output) == c)\n",
    "    \n",
    "    def classify(self, image):\n",
    "        output = self._processImage(image)\n",
    "        return np.argmax(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 0, Loss: 0, Accuracy: 0%\n",
      "Step: 100, Loss: 2.44349976717, Accuracy: 8%\n",
      "Step: 200, Loss: 2.30667343897, Accuracy: 17%\n",
      "Step: 300, Loss: 2.30487804472, Accuracy: 14%\n",
      "Step: 400, Loss: 2.29236341358, Accuracy: 13%\n",
      "Step: 500, Loss: 2.27364836326, Accuracy: 22%\n",
      "Step: 600, Loss: 2.26768354295, Accuracy: 16%\n",
      "Step: 700, Loss: 2.2728038169, Accuracy: 16%\n",
      "Step: 800, Loss: 2.23649550482, Accuracy: 21%\n",
      "Step: 900, Loss: 2.24007475566, Accuracy: 20%\n",
      "Step: 1000, Loss: 2.22294346872, Accuracy: 21%\n",
      "Step: 1100, Loss: 2.22828542948, Accuracy: 22%\n",
      "Step: 1200, Loss: 2.1900990903, Accuracy: 26%\n",
      "Step: 1300, Loss: 2.22701982157, Accuracy: 22%\n",
      "Step: 1400, Loss: 2.22898101871, Accuracy: 22%\n",
      "Step: 1500, Loss: 2.13920389747, Accuracy: 34%\n",
      "Step: 1600, Loss: 2.19037398608, Accuracy: 20%\n",
      "Step: 1700, Loss: 2.12003698964, Accuracy: 26%\n",
      "Step: 1800, Loss: 2.16548205066, Accuracy: 23%\n",
      "Step: 1900, Loss: 2.14350158851, Accuracy: 24%\n",
      "Step: 2000, Loss: 2.16767442975, Accuracy: 21%\n",
      "Step: 2100, Loss: 2.09130302404, Accuracy: 27%\n",
      "Step: 2200, Loss: 2.13486353533, Accuracy: 23%\n",
      "Step: 2300, Loss: 2.07661065815, Accuracy: 31%\n",
      "Step: 2400, Loss: 2.19376349386, Accuracy: 21%\n",
      "Step: 2500, Loss: 2.0973849642, Accuracy: 29%\n",
      "Step: 2600, Loss: 2.10138282725, Accuracy: 35%\n",
      "Step: 2700, Loss: 2.05867581662, Accuracy: 34%\n",
      "Step: 2800, Loss: 2.09952700927, Accuracy: 31%\n",
      "Step: 2900, Loss: 2.0679305363, Accuracy: 34%\n",
      "Step: 3000, Loss: 2.05976987039, Accuracy: 35%\n",
      "Step: 3100, Loss: 2.02583733357, Accuracy: 36%\n",
      "Step: 3200, Loss: 1.98417401864, Accuracy: 34%\n",
      "Step: 3300, Loss: 2.06415725538, Accuracy: 30%\n",
      "Step: 3400, Loss: 2.02984226145, Accuracy: 25%\n",
      "Step: 3500, Loss: 1.97073567046, Accuracy: 41%\n",
      "Step: 3600, Loss: 2.07918585141, Accuracy: 26%\n",
      "Step: 3700, Loss: 2.05339882657, Accuracy: 27%\n",
      "Step: 3800, Loss: 2.04639336314, Accuracy: 34%\n",
      "Step: 3900, Loss: 1.95094711874, Accuracy: 29%\n",
      "Step: 4000, Loss: 1.9070965245, Accuracy: 35%\n",
      "Step: 4100, Loss: 1.94212760813, Accuracy: 34%\n",
      "Step: 4200, Loss: 2.02163252253, Accuracy: 31%\n",
      "Step: 4300, Loss: 1.96956781805, Accuracy: 39%\n",
      "Step: 4400, Loss: 1.91293266425, Accuracy: 40%\n",
      "Step: 4500, Loss: 2.01614786102, Accuracy: 32%\n",
      "Step: 4600, Loss: 2.01801506942, Accuracy: 30%\n",
      "Step: 4700, Loss: 1.94697644412, Accuracy: 37%\n",
      "Step: 4800, Loss: 2.05984100042, Accuracy: 28%\n",
      "Step: 4900, Loss: 2.01210079466, Accuracy: 27%\n",
      "Step: 5000, Loss: 2.01206147635, Accuracy: 27%\n",
      "Step: 5100, Loss: 1.89306021904, Accuracy: 37%\n",
      "Step: 5200, Loss: 1.96514299238, Accuracy: 31%\n",
      "Step: 5300, Loss: 1.95562581691, Accuracy: 33%\n",
      "Step: 5400, Loss: 1.90670791286, Accuracy: 38%\n",
      "Step: 5500, Loss: 1.9996449586, Accuracy: 29%\n",
      "Step: 5600, Loss: 1.93770045784, Accuracy: 35%\n",
      "Step: 5700, Loss: 1.94491861953, Accuracy: 33%\n",
      "Step: 5800, Loss: 1.82582286334, Accuracy: 38%\n",
      "Step: 5900, Loss: 1.92259899013, Accuracy: 39%\n",
      "Step: 6000, Loss: 1.87833609914, Accuracy: 36%\n",
      "Step: 6100, Loss: 1.87040644917, Accuracy: 42%\n",
      "Step: 6200, Loss: 1.95159737759, Accuracy: 30%\n",
      "Step: 6300, Loss: 1.84320550414, Accuracy: 39%\n",
      "Step: 6400, Loss: 2.03196689435, Accuracy: 22%\n",
      "Step: 6500, Loss: 1.98265339404, Accuracy: 34%\n",
      "Step: 6600, Loss: 1.79713636149, Accuracy: 42%\n",
      "Step: 6700, Loss: 1.85876423105, Accuracy: 46%\n",
      "Step: 6800, Loss: 1.89405299704, Accuracy: 33%\n",
      "Step: 6900, Loss: 1.86576341148, Accuracy: 39%\n",
      "Step: 7000, Loss: 1.9874147901, Accuracy: 27%\n",
      "Step: 7100, Loss: 1.84714434328, Accuracy: 33%\n",
      "Step: 7200, Loss: 1.84644951906, Accuracy: 38%\n",
      "Step: 7300, Loss: 1.87270145359, Accuracy: 36%\n",
      "Step: 7400, Loss: 1.8477576013, Accuracy: 38%\n",
      "Step: 7500, Loss: 1.76138296385, Accuracy: 40%\n",
      "Step: 7600, Loss: 1.93189814743, Accuracy: 29%\n",
      "Step: 7700, Loss: 1.98266362367, Accuracy: 33%\n",
      "Step: 7800, Loss: 2.00292512549, Accuracy: 25%\n",
      "Step: 7900, Loss: 1.90241328744, Accuracy: 29%\n",
      "Step: 8000, Loss: 1.97124753846, Accuracy: 31%\n",
      "Step: 8100, Loss: 1.89522604802, Accuracy: 29%\n",
      "Step: 8200, Loss: 1.89981141525, Accuracy: 36%\n",
      "Step: 8300, Loss: 1.96147500502, Accuracy: 32%\n",
      "Step: 8400, Loss: 1.8383971243, Accuracy: 42%\n",
      "Step: 8500, Loss: 1.80465764176, Accuracy: 43%\n",
      "Step: 8600, Loss: 1.88388685735, Accuracy: 35%\n",
      "Step: 8700, Loss: 1.78043829771, Accuracy: 46%\n",
      "Step: 8800, Loss: 2.05142943758, Accuracy: 25%\n",
      "Step: 8900, Loss: 1.88928765455, Accuracy: 34%\n",
      "Step: 9000, Loss: 1.9914375462, Accuracy: 26%\n",
      "Step: 9100, Loss: 1.85254009908, Accuracy: 39%\n",
      "Step: 9200, Loss: 1.91195078451, Accuracy: 27%\n",
      "Step: 9300, Loss: 1.96533784324, Accuracy: 34%\n",
      "Step: 9400, Loss: 1.68957285018, Accuracy: 44%\n",
      "Step: 9500, Loss: 1.94380395608, Accuracy: 27%\n",
      "Step: 9600, Loss: 1.87507381984, Accuracy: 41%\n",
      "Step: 9700, Loss: 1.91605957813, Accuracy: 32%\n",
      "Step: 9800, Loss: 1.83053455909, Accuracy: 37%\n",
      "Step: 9900, Loss: 1.83641889376, Accuracy: 31%\n",
      "Step: 10000, Loss: 1.76054636317, Accuracy: 37%\n",
      "Step: 10100, Loss: 1.87010481825, Accuracy: 38%\n",
      "Step: 10200, Loss: 1.93184442643, Accuracy: 33%\n",
      "Step: 10300, Loss: 1.84355655457, Accuracy: 39%\n",
      "Step: 10400, Loss: 1.97613003391, Accuracy: 39%\n",
      "Step: 10500, Loss: 1.78272711289, Accuracy: 38%\n",
      "Step: 10600, Loss: 1.86565531931, Accuracy: 33%\n",
      "Step: 10700, Loss: 1.838749148, Accuracy: 37%\n",
      "Step: 10800, Loss: 1.97361411634, Accuracy: 26%\n",
      "Step: 10900, Loss: 1.82268755913, Accuracy: 34%\n",
      "Step: 11000, Loss: 1.86600848573, Accuracy: 41%\n",
      "Step: 11100, Loss: 1.98232929031, Accuracy: 28%\n",
      "Step: 11200, Loss: 1.96711356279, Accuracy: 32%\n",
      "Step: 11300, Loss: 1.94893978356, Accuracy: 32%\n",
      "Step: 11400, Loss: 2.00089131195, Accuracy: 31%\n",
      "Step: 11500, Loss: 1.88877251699, Accuracy: 40%\n",
      "Step: 11600, Loss: 1.76069131625, Accuracy: 41%\n",
      "Step: 11700, Loss: 1.74802656442, Accuracy: 41%\n",
      "Step: 11800, Loss: 1.95130186954, Accuracy: 30%\n",
      "Step: 11900, Loss: 1.79113454415, Accuracy: 39%\n",
      "Step: 12000, Loss: 1.88640320098, Accuracy: 35%\n",
      "Step: 12100, Loss: 2.03832865508, Accuracy: 28%\n",
      "Step: 12200, Loss: 1.85761083868, Accuracy: 39%\n",
      "Step: 12300, Loss: 1.94488593447, Accuracy: 39%\n",
      "Step: 12400, Loss: 1.80611778114, Accuracy: 33%\n",
      "Step: 12500, Loss: 1.88757823874, Accuracy: 37%\n",
      "Step: 12600, Loss: 1.77058097116, Accuracy: 46%\n",
      "Step: 12700, Loss: 1.92560250481, Accuracy: 29%\n",
      "Step: 12800, Loss: 1.79105436306, Accuracy: 30%\n",
      "Step: 12900, Loss: 1.87450248113, Accuracy: 34%\n",
      "Step: 13000, Loss: 1.93475806322, Accuracy: 27%\n",
      "Step: 13100, Loss: 1.70150933106, Accuracy: 44%\n",
      "Step: 13200, Loss: 1.78625484515, Accuracy: 42%\n",
      "Step: 13300, Loss: 1.73567788303, Accuracy: 39%\n",
      "Step: 13400, Loss: 1.81587630267, Accuracy: 37%\n",
      "Step: 13500, Loss: 1.87229036081, Accuracy: 29%\n",
      "Step: 13600, Loss: 1.85336800677, Accuracy: 40%\n",
      "Step: 13700, Loss: 1.96298212116, Accuracy: 31%\n",
      "Step: 13800, Loss: 1.89488490564, Accuracy: 32%\n",
      "Step: 13900, Loss: 1.85530374287, Accuracy: 38%\n",
      "Step: 14000, Loss: 1.76180973979, Accuracy: 38%\n",
      "Step: 14100, Loss: 1.79662388301, Accuracy: 37%\n",
      "Step: 14200, Loss: 1.81413057225, Accuracy: 38%\n",
      "Step: 14300, Loss: 1.86951386791, Accuracy: 30%\n",
      "Step: 14400, Loss: 1.81034610848, Accuracy: 33%\n",
      "Step: 14500, Loss: 1.72057263959, Accuracy: 44%\n",
      "Step: 14600, Loss: 1.74756582855, Accuracy: 43%\n",
      "Step: 14700, Loss: 1.97061685425, Accuracy: 31%\n",
      "Step: 14800, Loss: 1.59142391559, Accuracy: 49%\n",
      "Step: 14900, Loss: 1.81756774575, Accuracy: 37%\n",
      "Step: 15000, Loss: 1.80557344839, Accuracy: 44%\n",
      "Step: 15100, Loss: 1.8264409714, Accuracy: 36%\n",
      "Step: 15200, Loss: 1.75550551492, Accuracy: 37%\n",
      "Step: 15300, Loss: 1.79873591068, Accuracy: 46%\n",
      "Step: 15400, Loss: 1.88140994005, Accuracy: 40%\n",
      "Step: 15500, Loss: 1.92445804219, Accuracy: 32%\n",
      "Step: 15600, Loss: 1.85610617557, Accuracy: 30%\n",
      "Step: 15700, Loss: 1.74508060754, Accuracy: 38%\n",
      "Step: 15800, Loss: 1.92471363935, Accuracy: 28%\n",
      "Step: 15900, Loss: 1.85943775274, Accuracy: 40%\n",
      "Step: 16000, Loss: 1.97235855159, Accuracy: 32%\n",
      "Step: 16100, Loss: 2.00910520163, Accuracy: 29%\n",
      "Step: 16200, Loss: 1.76761262532, Accuracy: 48%\n",
      "Step: 16300, Loss: 1.6562742253, Accuracy: 42%\n",
      "Step: 16400, Loss: 1.9020275724, Accuracy: 29%\n",
      "Step: 16500, Loss: 1.9094202297, Accuracy: 37%\n",
      "Step: 16600, Loss: 1.84709175201, Accuracy: 30%\n",
      "Step: 16700, Loss: 1.78385583024, Accuracy: 38%\n",
      "Step: 16800, Loss: 1.73936373695, Accuracy: 36%\n",
      "Step: 16900, Loss: 1.75786412272, Accuracy: 34%\n",
      "Step: 17000, Loss: 1.83460959508, Accuracy: 35%\n",
      "Step: 17100, Loss: 1.80649970754, Accuracy: 32%\n",
      "Step: 17200, Loss: 1.91278542878, Accuracy: 30%\n",
      "Step: 17300, Loss: 1.72973549992, Accuracy: 42%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 17400, Loss: 1.71072843861, Accuracy: 40%\n",
      "Step: 17500, Loss: 1.89547249071, Accuracy: 31%\n",
      "Step: 17600, Loss: 2.00675975901, Accuracy: 32%\n",
      "Step: 17700, Loss: 1.84190730132, Accuracy: 35%\n",
      "Step: 17800, Loss: 1.95845464209, Accuracy: 31%\n",
      "Step: 17900, Loss: 1.85317100716, Accuracy: 38%\n",
      "Step: 18000, Loss: 1.80350326605, Accuracy: 38%\n",
      "Step: 18100, Loss: 1.80346183479, Accuracy: 38%\n",
      "Step: 18200, Loss: 1.89464649721, Accuracy: 40%\n",
      "Step: 18300, Loss: 1.80628849595, Accuracy: 36%\n",
      "Step: 18400, Loss: 1.71983029448, Accuracy: 45%\n",
      "Step: 18500, Loss: 1.96506356244, Accuracy: 29%\n",
      "Step: 18600, Loss: 1.82839244021, Accuracy: 28%\n",
      "Step: 18700, Loss: 1.59271477648, Accuracy: 42%\n",
      "Step: 18800, Loss: 1.7763437486, Accuracy: 35%\n",
      "Step: 18900, Loss: 1.88392436256, Accuracy: 38%\n",
      "Step: 19000, Loss: 1.8256401032, Accuracy: 33%\n",
      "Step: 19100, Loss: 1.77275531424, Accuracy: 39%\n",
      "Step: 19200, Loss: 1.74276245583, Accuracy: 35%\n",
      "Step: 19300, Loss: 1.93540698677, Accuracy: 29%\n",
      "Step: 19400, Loss: 1.8328771819, Accuracy: 32%\n",
      "Step: 19500, Loss: 1.81481990932, Accuracy: 29%\n",
      "Step: 19600, Loss: 1.81581178091, Accuracy: 39%\n",
      "Step: 19700, Loss: 1.86476573611, Accuracy: 30%\n",
      "Step: 19800, Loss: 1.83296026844, Accuracy: 34%\n",
      "Step: 19900, Loss: 1.73856505299, Accuracy: 44%\n",
      "Step: 20000, Loss: 1.73891182867, Accuracy: 44%\n",
      "Step: 20100, Loss: 1.73992098827, Accuracy: 35%\n",
      "Step: 20200, Loss: 1.67130177522, Accuracy: 46%\n",
      "Step: 20300, Loss: 1.88758439703, Accuracy: 35%\n",
      "Step: 20400, Loss: 1.83932454183, Accuracy: 36%\n",
      "Step: 20500, Loss: 1.79763887698, Accuracy: 34%\n",
      "Step: 20600, Loss: 1.74409168539, Accuracy: 39%\n",
      "Step: 20700, Loss: 1.70877426305, Accuracy: 42%\n",
      "Step: 20800, Loss: 2.00346712532, Accuracy: 30%\n",
      "Step: 20900, Loss: 1.77830585251, Accuracy: 35%\n",
      "Step: 21000, Loss: 2.09690465088, Accuracy: 25%\n",
      "Step: 21100, Loss: 1.7982902445, Accuracy: 32%\n",
      "Step: 21200, Loss: 1.85932536736, Accuracy: 39%\n",
      "Step: 21300, Loss: 1.729384187, Accuracy: 42%\n",
      "Step: 21400, Loss: 1.70891764001, Accuracy: 38%\n",
      "Step: 21500, Loss: 1.73717475975, Accuracy: 38%\n",
      "Step: 21600, Loss: 1.76781522732, Accuracy: 37%\n",
      "Step: 21700, Loss: 2.03919483691, Accuracy: 28%\n",
      "Step: 21800, Loss: 1.88165717247, Accuracy: 32%\n",
      "Step: 21900, Loss: 1.85841593249, Accuracy: 31%\n",
      "Step: 22000, Loss: 1.83985890289, Accuracy: 41%\n",
      "Step: 22100, Loss: 1.78716554442, Accuracy: 31%\n",
      "Step: 22200, Loss: 1.87535619759, Accuracy: 31%\n",
      "Step: 22300, Loss: 1.78722663574, Accuracy: 36%\n",
      "Step: 22400, Loss: 1.64253067233, Accuracy: 43%\n",
      "Step: 22500, Loss: 1.82578291602, Accuracy: 38%\n",
      "Step: 22600, Loss: 1.72250192616, Accuracy: 38%\n",
      "Step: 22700, Loss: 1.78352143926, Accuracy: 36%\n",
      "Step: 22800, Loss: 1.76668547302, Accuracy: 38%\n",
      "Step: 22900, Loss: 1.76625483197, Accuracy: 35%\n",
      "Step: 23000, Loss: 2.00447920376, Accuracy: 26%\n",
      "Step: 23100, Loss: 1.80971650712, Accuracy: 41%\n",
      "Step: 23200, Loss: 1.6345279529, Accuracy: 45%\n",
      "Step: 23300, Loss: 1.77617226676, Accuracy: 37%\n",
      "Step: 23400, Loss: 1.81029863001, Accuracy: 34%\n",
      "Step: 23500, Loss: 1.96627492585, Accuracy: 30%\n",
      "Step: 23600, Loss: 1.82230568898, Accuracy: 38%\n",
      "Step: 23700, Loss: 1.72944533533, Accuracy: 34%\n",
      "Step: 23800, Loss: 1.75392067866, Accuracy: 38%\n",
      "Step: 23900, Loss: 1.77603988411, Accuracy: 45%\n",
      "Step: 24000, Loss: 1.86617605834, Accuracy: 31%\n",
      "Step: 24100, Loss: 1.8065805235, Accuracy: 36%\n",
      "Step: 24200, Loss: 1.6855135176, Accuracy: 41%\n",
      "Step: 24300, Loss: 1.95899653847, Accuracy: 34%\n",
      "Step: 24400, Loss: 1.87747257764, Accuracy: 32%\n",
      "Step: 24500, Loss: 1.92479070044, Accuracy: 38%\n",
      "Step: 24600, Loss: 1.65557748857, Accuracy: 39%\n",
      "Step: 24700, Loss: 1.84642572119, Accuracy: 36%\n",
      "Step: 24800, Loss: 1.90843258946, Accuracy: 34%\n",
      "Step: 24900, Loss: 1.90406467555, Accuracy: 34%\n",
      "Step: 25000, Loss: 1.91691465979, Accuracy: 36%\n",
      "Step: 25100, Loss: 1.8135501005, Accuracy: 33%\n",
      "Step: 25200, Loss: 1.99076315633, Accuracy: 26%\n",
      "Step: 25300, Loss: 1.77596169554, Accuracy: 36%\n",
      "Step: 25400, Loss: 1.78749573593, Accuracy: 39%\n",
      "Step: 25500, Loss: 1.65253805376, Accuracy: 40%\n",
      "Step: 25600, Loss: 1.73940521744, Accuracy: 37%\n",
      "Step: 25700, Loss: 1.76970874061, Accuracy: 36%\n",
      "Step: 25800, Loss: 1.86843915039, Accuracy: 36%\n",
      "Step: 25900, Loss: 1.81028593094, Accuracy: 36%\n",
      "Step: 26000, Loss: 1.69613888085, Accuracy: 40%\n",
      "Step: 26100, Loss: 1.62904954789, Accuracy: 39%\n",
      "Step: 26200, Loss: 1.79097931752, Accuracy: 37%\n",
      "Step: 26300, Loss: 1.74959700171, Accuracy: 39%\n",
      "Step: 26400, Loss: 1.76189164494, Accuracy: 42%\n",
      "Step: 26500, Loss: 1.74147783542, Accuracy: 40%\n",
      "Step: 26600, Loss: 1.78324893328, Accuracy: 36%\n",
      "Step: 26700, Loss: 1.80240093529, Accuracy: 35%\n",
      "Step: 26800, Loss: 1.97335809875, Accuracy: 27%\n",
      "Step: 26900, Loss: 1.76430517072, Accuracy: 36%\n",
      "Step: 27000, Loss: 1.85150630506, Accuracy: 37%\n",
      "Step: 27100, Loss: 1.72211076123, Accuracy: 40%\n",
      "Step: 27200, Loss: 1.85440758663, Accuracy: 33%\n",
      "Step: 27300, Loss: 1.84411109157, Accuracy: 38%\n",
      "Step: 27400, Loss: 1.77505573153, Accuracy: 40%\n",
      "Step: 27500, Loss: 1.85167723145, Accuracy: 28%\n",
      "Step: 27600, Loss: 1.846508223, Accuracy: 33%\n",
      "Step: 27700, Loss: 1.85236634649, Accuracy: 34%\n",
      "Step: 27800, Loss: 1.7988434405, Accuracy: 42%\n",
      "Step: 27900, Loss: 1.71900683864, Accuracy: 37%\n",
      "Step: 28000, Loss: 1.89499192348, Accuracy: 37%\n",
      "Step: 28100, Loss: 1.85092277921, Accuracy: 33%\n",
      "Step: 28200, Loss: 1.65458983466, Accuracy: 43%\n",
      "Step: 28300, Loss: 1.75141043961, Accuracy: 35%\n",
      "Step: 28400, Loss: 1.60988733172, Accuracy: 46%\n",
      "Step: 28500, Loss: 1.84006544689, Accuracy: 34%\n",
      "Step: 28600, Loss: 1.8416297072, Accuracy: 37%\n",
      "Step: 28700, Loss: 1.77769896786, Accuracy: 33%\n",
      "Step: 28800, Loss: 1.71957787392, Accuracy: 38%\n",
      "Step: 28900, Loss: 1.89614223627, Accuracy: 35%\n",
      "Step: 29000, Loss: 1.75607298731, Accuracy: 43%\n",
      "Step: 29100, Loss: 1.7580843066, Accuracy: 41%\n",
      "Step: 29200, Loss: 1.9793946678, Accuracy: 34%\n",
      "Step: 29300, Loss: 1.80421485334, Accuracy: 41%\n",
      "Step: 29400, Loss: 1.85142971851, Accuracy: 33%\n",
      "Step: 29500, Loss: 1.9076590226, Accuracy: 29%\n",
      "Step: 29600, Loss: 1.99513232964, Accuracy: 29%\n",
      "Step: 29700, Loss: 1.86028263248, Accuracy: 34%\n",
      "Step: 29800, Loss: 1.73328897224, Accuracy: 40%\n",
      "Step: 29900, Loss: 1.81368471737, Accuracy: 41%\n",
      "Step: 30000, Loss: 1.68488048467, Accuracy: 38%\n",
      "Step: 30100, Loss: 1.77747599472, Accuracy: 33%\n",
      "Step: 30200, Loss: 1.71691452346, Accuracy: 37%\n",
      "Step: 30300, Loss: 1.65402499222, Accuracy: 44%\n",
      "Step: 30400, Loss: 1.86621067773, Accuracy: 28%\n",
      "Step: 30500, Loss: 1.76614053428, Accuracy: 40%\n",
      "Step: 30600, Loss: 1.89339822661, Accuracy: 29%\n",
      "Step: 30700, Loss: 1.64351810233, Accuracy: 45%\n",
      "Step: 30800, Loss: 1.91747940183, Accuracy: 35%\n",
      "Step: 30900, Loss: 1.84058515901, Accuracy: 36%\n",
      "Step: 31000, Loss: 1.78584002356, Accuracy: 32%\n",
      "Step: 31100, Loss: 1.5606172859, Accuracy: 46%\n",
      "Step: 31200, Loss: 1.79177161742, Accuracy: 34%\n",
      "Step: 31300, Loss: 1.7090858972, Accuracy: 43%\n",
      "Step: 31400, Loss: 1.95897265241, Accuracy: 36%\n",
      "Step: 31500, Loss: 1.84607531575, Accuracy: 39%\n",
      "Step: 31600, Loss: 1.86894527425, Accuracy: 37%\n",
      "Step: 31700, Loss: 1.96736386185, Accuracy: 32%\n",
      "Step: 31800, Loss: 1.85395727909, Accuracy: 34%\n",
      "Step: 31900, Loss: 1.90754862777, Accuracy: 36%\n",
      "Step: 32000, Loss: 1.81722647759, Accuracy: 36%\n",
      "Step: 32100, Loss: 1.85164131576, Accuracy: 34%\n",
      "Step: 32200, Loss: 1.66954323541, Accuracy: 44%\n",
      "Step: 32300, Loss: 1.63900086364, Accuracy: 39%\n",
      "Step: 32400, Loss: 1.67549420746, Accuracy: 39%\n",
      "Step: 32500, Loss: 1.73281121303, Accuracy: 34%\n",
      "Step: 32600, Loss: 1.88407959473, Accuracy: 37%\n",
      "Step: 32700, Loss: 1.84112096266, Accuracy: 33%\n",
      "Step: 32800, Loss: 1.94861413931, Accuracy: 30%\n",
      "Step: 32900, Loss: 1.71914196055, Accuracy: 40%\n",
      "Step: 33000, Loss: 1.79137205435, Accuracy: 35%\n",
      "Step: 33100, Loss: 1.93416148814, Accuracy: 33%\n",
      "Step: 33200, Loss: 2.07712393014, Accuracy: 22%\n",
      "Step: 33300, Loss: 1.98271288982, Accuracy: 33%\n",
      "Step: 33400, Loss: 1.8189986654, Accuracy: 42%\n",
      "Step: 33500, Loss: 1.86567888997, Accuracy: 34%\n",
      "Step: 33600, Loss: 1.86482373011, Accuracy: 28%\n",
      "Step: 33700, Loss: 1.87972709431, Accuracy: 35%\n",
      "Step: 33800, Loss: 1.64012679349, Accuracy: 45%\n",
      "Step: 33900, Loss: 1.75084152553, Accuracy: 39%\n",
      "Step: 34000, Loss: 1.77716532258, Accuracy: 39%\n",
      "Step: 34100, Loss: 1.66858272976, Accuracy: 43%\n",
      "Step: 34200, Loss: 1.76542785043, Accuracy: 35%\n",
      "Step: 34300, Loss: 1.66620084204, Accuracy: 42%\n",
      "Step: 34400, Loss: 1.62015252266, Accuracy: 47%\n",
      "Step: 34500, Loss: 1.81175929994, Accuracy: 43%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 34600, Loss: 1.82906802164, Accuracy: 41%\n",
      "Step: 34700, Loss: 1.92321003635, Accuracy: 30%\n",
      "Step: 34800, Loss: 1.86874743769, Accuracy: 40%\n",
      "Step: 34900, Loss: 1.67339996706, Accuracy: 37%\n",
      "Step: 35000, Loss: 1.78848416369, Accuracy: 35%\n",
      "Step: 35100, Loss: 1.83537415466, Accuracy: 35%\n",
      "Step: 35200, Loss: 1.82701385936, Accuracy: 39%\n",
      "Step: 35300, Loss: 1.92034779759, Accuracy: 33%\n",
      "Step: 35400, Loss: 1.83134292767, Accuracy: 35%\n",
      "Step: 35500, Loss: 1.89508169439, Accuracy: 31%\n",
      "Step: 35600, Loss: 1.75950385225, Accuracy: 42%\n",
      "Step: 35700, Loss: 1.80153935213, Accuracy: 34%\n",
      "Step: 35800, Loss: 1.59196688204, Accuracy: 44%\n",
      "Step: 35900, Loss: 1.93847458801, Accuracy: 34%\n",
      "Step: 36000, Loss: 1.96001154576, Accuracy: 30%\n",
      "Step: 36100, Loss: 1.54742937696, Accuracy: 43%\n",
      "Step: 36200, Loss: 1.72628179946, Accuracy: 39%\n",
      "Step: 36300, Loss: 1.75199463555, Accuracy: 35%\n",
      "Step: 36400, Loss: 1.67571044327, Accuracy: 42%\n",
      "Step: 36500, Loss: 1.67722914318, Accuracy: 41%\n",
      "Step: 36600, Loss: 1.89221036656, Accuracy: 31%\n",
      "Step: 36700, Loss: 1.65292168937, Accuracy: 40%\n",
      "Step: 36800, Loss: 1.78759018048, Accuracy: 39%\n",
      "Step: 36900, Loss: 1.79721428279, Accuracy: 28%\n",
      "Step: 37000, Loss: 1.80406173017, Accuracy: 34%\n",
      "Step: 37100, Loss: 1.72242577459, Accuracy: 36%\n",
      "Step: 37200, Loss: 1.78645340002, Accuracy: 36%\n",
      "Step: 37300, Loss: 1.74084706663, Accuracy: 40%\n",
      "Step: 37400, Loss: 1.81097848663, Accuracy: 35%\n",
      "Step: 37500, Loss: 1.75858424834, Accuracy: 38%\n",
      "Step: 37600, Loss: 1.75655022623, Accuracy: 42%\n",
      "Step: 37700, Loss: 1.73422599298, Accuracy: 36%\n",
      "Step: 37800, Loss: 1.75206880595, Accuracy: 34%\n",
      "Step: 37900, Loss: 1.72137921292, Accuracy: 38%\n",
      "Step: 38000, Loss: 1.71609640541, Accuracy: 38%\n",
      "Step: 38100, Loss: 1.88952950348, Accuracy: 28%\n",
      "Step: 38200, Loss: 1.82886774225, Accuracy: 32%\n",
      "Step: 38300, Loss: 1.59106431782, Accuracy: 43%\n",
      "Step: 38400, Loss: 1.95297773681, Accuracy: 31%\n",
      "Step: 38500, Loss: 1.73611215591, Accuracy: 41%\n",
      "Step: 38600, Loss: 1.88620185469, Accuracy: 37%\n",
      "Step: 38700, Loss: 1.51841345091, Accuracy: 47%\n",
      "Step: 38800, Loss: 1.70652351201, Accuracy: 42%\n",
      "Step: 38900, Loss: 1.83467918975, Accuracy: 38%\n",
      "Step: 39000, Loss: 1.80339866347, Accuracy: 32%\n",
      "Step: 39100, Loss: 1.90101255905, Accuracy: 38%\n",
      "Step: 39200, Loss: 1.68217644588, Accuracy: 46%\n",
      "Step: 39300, Loss: 2.01666556198, Accuracy: 37%\n",
      "Step: 39400, Loss: 1.76777403692, Accuracy: 35%\n",
      "Step: 39500, Loss: 1.86512911127, Accuracy: 26%\n",
      "Step: 39600, Loss: 1.72936214518, Accuracy: 42%\n",
      "Step: 39700, Loss: 1.82107326826, Accuracy: 35%\n",
      "Step: 39800, Loss: 1.85597029498, Accuracy: 37%\n",
      "Step: 39900, Loss: 1.94083284735, Accuracy: 34%\n",
      "Step: 40000, Loss: 1.9235939229, Accuracy: 32%\n",
      "Step: 40100, Loss: 1.78701538275, Accuracy: 37%\n",
      "Step: 40200, Loss: 2.13577119727, Accuracy: 29%\n",
      "Step: 40300, Loss: 2.10934291383, Accuracy: 27%\n",
      "Step: 40400, Loss: 2.10074070769, Accuracy: 35%\n",
      "Step: 40500, Loss: 1.70248975266, Accuracy: 38%\n",
      "Step: 40600, Loss: 1.76494843116, Accuracy: 35%\n",
      "Step: 40700, Loss: 1.72773082689, Accuracy: 36%\n",
      "Step: 40800, Loss: 1.8760773707, Accuracy: 35%\n",
      "Step: 40900, Loss: 1.76865570534, Accuracy: 35%\n",
      "Step: 41000, Loss: 1.85877452918, Accuracy: 37%\n",
      "Step: 41100, Loss: 1.92280472519, Accuracy: 36%\n",
      "Step: 41200, Loss: 1.74991373444, Accuracy: 41%\n",
      "Step: 41300, Loss: 1.94698553165, Accuracy: 31%\n",
      "Step: 41400, Loss: 1.7826703011, Accuracy: 35%\n",
      "Step: 41500, Loss: 1.82801562358, Accuracy: 34%\n",
      "Step: 41600, Loss: 1.85669067439, Accuracy: 38%\n",
      "Step: 41700, Loss: 1.91756682916, Accuracy: 31%\n",
      "Step: 41800, Loss: 1.79072417779, Accuracy: 39%\n",
      "Step: 41900, Loss: 1.85380102876, Accuracy: 39%\n",
      "Step: 42000, Loss: 1.84005331349, Accuracy: 34%\n",
      "Step: 42100, Loss: 1.88319577639, Accuracy: 33%\n",
      "Step: 42200, Loss: 1.81688717584, Accuracy: 35%\n",
      "Step: 42300, Loss: 1.83899965682, Accuracy: 39%\n",
      "Step: 42400, Loss: 1.74430931656, Accuracy: 37%\n",
      "Step: 42500, Loss: 1.76385418989, Accuracy: 39%\n",
      "Step: 42600, Loss: 1.76895014796, Accuracy: 37%\n",
      "Step: 42700, Loss: 1.92971036685, Accuracy: 32%\n",
      "Step: 42800, Loss: 1.91875603464, Accuracy: 36%\n",
      "Step: 42900, Loss: 1.8486252315, Accuracy: 35%\n",
      "Step: 43000, Loss: 1.62359418487, Accuracy: 47%\n",
      "Step: 43100, Loss: 1.72682914183, Accuracy: 44%\n",
      "Step: 43200, Loss: 1.77035030962, Accuracy: 47%\n",
      "Step: 43300, Loss: 1.82516892567, Accuracy: 27%\n",
      "Step: 43400, Loss: 1.85571395337, Accuracy: 32%\n",
      "Step: 43500, Loss: 1.94620208298, Accuracy: 31%\n",
      "Step: 43600, Loss: 1.73174472524, Accuracy: 42%\n",
      "Step: 43700, Loss: 1.84823191181, Accuracy: 33%\n",
      "Step: 43800, Loss: 1.82819732864, Accuracy: 37%\n",
      "Step: 43900, Loss: 1.90376861742, Accuracy: 37%\n",
      "Step: 44000, Loss: 1.79326420678, Accuracy: 31%\n",
      "Step: 44100, Loss: 1.91591719031, Accuracy: 29%\n",
      "Step: 44200, Loss: 1.85647229676, Accuracy: 33%\n",
      "Step: 44300, Loss: 1.84688482598, Accuracy: 36%\n",
      "Step: 44400, Loss: 1.72584004126, Accuracy: 38%\n",
      "Step: 44500, Loss: 1.80871798775, Accuracy: 34%\n",
      "Step: 44600, Loss: 1.90356048917, Accuracy: 32%\n",
      "Step: 44700, Loss: 1.81412612865, Accuracy: 34%\n",
      "Step: 44800, Loss: 1.81805983149, Accuracy: 30%\n",
      "Step: 44900, Loss: 1.87067512051, Accuracy: 30%\n",
      "Step: 45000, Loss: 1.78896748763, Accuracy: 36%\n",
      "Step: 45100, Loss: 1.69196786728, Accuracy: 41%\n",
      "Step: 45200, Loss: 1.86288306677, Accuracy: 36%\n",
      "Step: 45300, Loss: 1.64249069729, Accuracy: 42%\n",
      "Step: 45400, Loss: 1.70386841918, Accuracy: 41%\n",
      "Step: 45500, Loss: 1.78186555746, Accuracy: 38%\n",
      "Step: 45600, Loss: 1.69418850311, Accuracy: 39%\n",
      "Step: 45700, Loss: 1.73703609501, Accuracy: 41%\n",
      "Step: 45800, Loss: 1.76639321007, Accuracy: 34%\n",
      "Step: 45900, Loss: 1.8487979384, Accuracy: 30%\n",
      "Step: 46000, Loss: 1.69875510359, Accuracy: 40%\n",
      "Step: 46100, Loss: 1.9515987414, Accuracy: 30%\n",
      "Step: 46200, Loss: 1.76753303483, Accuracy: 38%\n",
      "Step: 46300, Loss: 1.69097544084, Accuracy: 38%\n",
      "Step: 46400, Loss: 1.63629149234, Accuracy: 39%\n",
      "Step: 46500, Loss: 1.6493173282, Accuracy: 42%\n",
      "Step: 46600, Loss: 1.66005534818, Accuracy: 41%\n",
      "Step: 46700, Loss: 1.71186347241, Accuracy: 38%\n",
      "Step: 46800, Loss: 1.72754720515, Accuracy: 40%\n",
      "Step: 46900, Loss: 1.62863963634, Accuracy: 40%\n",
      "Step: 47000, Loss: 1.90246183022, Accuracy: 33%\n",
      "Step: 47100, Loss: 1.72450461644, Accuracy: 37%\n",
      "Step: 47200, Loss: 1.81355187381, Accuracy: 39%\n"
     ]
    }
   ],
   "source": [
    "conv = Conv3x3(D, Adam())                      # H x W x 1 --> H-2 x W-2 x D\n",
    "pool = MaxPool2()                              # H-2 x W-2 x D --> (H-2)/2 x (W-2)/2 x D\n",
    "softmax = Softmax(((H-2) // 2) * ((W-2) // 2) * D, LABELS, Adam) # (H-2)/W x (W-2)/2 x D --> (H-2)/2*(W-2)/w*D\n",
    "\n",
    "m = Model(conv, pool, softmax)\n",
    "permutation = np.random.permutation(len(train_images))\n",
    "m.fit(train_images[permutation], train_labels[permutation])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 20%\n"
     ]
    }
   ],
   "source": [
    "prediction = [m.classify(image) for image in test_images]\n",
    "\n",
    "\n",
    "testCount = len(test_images)\n",
    "correct = sum([prediction[i] == test_labels[i] for i in range(len(prediction))])\n",
    "print('Accuracy: {0}%'.format(correct * 100 / testCount))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   0   1  2  3   4   5   6   7  8   9\n",
      "0  0   4  0  0   2   5  70   4  0   0\n",
      "1  0  50  0  0   0   4  66   1  2   3\n",
      "2  0  13  0  0   1   3  95   2  2   0\n",
      "3  0  10  0  0   3   8  80   3  0   3\n",
      "4  0  13  0  0  16   3  70   8  0   0\n",
      "5  0  18  0  0   1  12  51   3  1   1\n",
      "6  0   5  0  0   1   1  80   0  0   0\n",
      "7  0   4  0  0   2   1  67  19  2   4\n",
      "8  0  16  0  0   3   8  51   2  5   4\n",
      "9  0   6  0  0   1   0  61   5  0  21\n"
     ]
    }
   ],
   "source": [
    "cm = [[0 for j in range(10)] for i in range(10)]\n",
    "for p, l in zip(prediction, test_labels):\n",
    "    cm[l][p] += 1\n",
    "\n",
    "print(DataFrame(cm))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train_fmnist, y_train_fmnist), (x_test_fmnist, y_test_fmnist) = fashion_mnist.load_data()\n",
    "\n",
    "x_train_fmnist = x_train_fmnist[:1000]\n",
    "y_train_fmnist = y_train_fmnist[:1000]\n",
    "x_test_fmnist = x_train_fmnist[:1000]\n",
    "y_test_fmnist = y_train_fmnist[:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 0, Loss: 0, Accuracy: 0%\n",
      "Step: 100, Loss: 2.31526482629, Accuracy: 11%\n",
      "Step: 200, Loss: 2.28081805383, Accuracy: 11%\n",
      "Step: 300, Loss: 2.21332384969, Accuracy: 23%\n",
      "Step: 400, Loss: 2.20124129264, Accuracy: 23%\n",
      "Step: 500, Loss: 2.11493267216, Accuracy: 26%\n",
      "Step: 600, Loss: 2.15959366146, Accuracy: 25%\n",
      "Step: 700, Loss: 2.20439999548, Accuracy: 21%\n",
      "Step: 800, Loss: 2.16469881401, Accuracy: 26%\n",
      "Step: 900, Loss: 2.07221354315, Accuracy: 26%\n"
     ]
    }
   ],
   "source": [
    "m = Model(conv, pool, softmax)\n",
    "permutation = np.random.permutation(len(x_train_fmnist))\n",
    "m.fit(x_train_fmnist[permutation], y_train_fmnist[permutation])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 33%\n"
     ]
    }
   ],
   "source": [
    "prediction = [m.classify(image) for image in x_test_fmnist]\n",
    "\n",
    "\n",
    "testCount = len(test_images)\n",
    "correct = sum([prediction[i] == y_test_fmnist[i] for i in range(len(prediction))])\n",
    "print('Accuracy: {0}%'.format(correct * 100 / testCount))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   0   1   2   3   4  5   6   7   8   9\n",
      "0  8  16   1   5   1  0  22  10  43   1\n",
      "1  0  55   1   0   0  0   3  14  31   0\n",
      "2  0   6  19   1   5  0   8  16  31   0\n",
      "3  0   9   0  19   1  0   4  17  42   0\n",
      "4  0   7   4   0  18  0  16   9  41   0\n",
      "5  0   3   0   0   0  0   0  51  43   3\n",
      "6  1  13   2   3   5  0  21  12  42   1\n",
      "7  0   4   0   0   0  0   0  88  23   0\n",
      "8  0   2   0   0   1  0   1  12  86   0\n",
      "9  0   3   0   0   0  0   0  24  51  21\n"
     ]
    }
   ],
   "source": [
    "cm = [[0 for j in range(10)] for i in range(10)]\n",
    "for p, l in zip(prediction, y_test_fmnist):\n",
    "    cm[l][p] += 1\n",
    "\n",
    "print(DataFrame(cm))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
